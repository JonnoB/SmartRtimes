---
title: "Untitled"
author: "Jonathan Bourne"
date: "8 May 2017"
output: html_document
---


I need chunked loader
time filterer <- Make a change to the function that allows chunked or whole mode.
add in functionality for  different kinds of filtering.



```{r Packages}
packages <-c("stringr", "lubridate", "data.table", "R.utils", "corrplot", "Matrix", "ff", "zoo", "parallel", "tidyverse")

new.packages <- packages[!(packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

sapply(packages, library, character.only = TRUE)
rm(list=c("packages",  "new.packages"))
  

isAWS <-(Sys.info()[1]=="Linux")

```

Project Folders
```{r Paths}
#basewd needs to be changed
if(isAWS){
  basewd<- file.path(getwd(),"Dropbox/Thesis-Data")

      functioncode <- file.path(basewd, "SmartMeterThesisCode","Functions")
  } else {
  
  basewd <- "C:/Users/pc1/Dropbox/Thesis-Data"
  functioncode <- "C:/Users/pc1/Dropbox/Thesis-Data/SmartMeterThesisCode/Functions"
  }

SubDataSets <- file.path(basewd, "SubDataSets")
datafile <- file.path(basewd, "TCa1")

```

Source functions
```{r Functions}
setwd("/home/jonno/SmartRtimes/Functions")
sapply(list.files(), source)

setwd(file.path("/home/jonno/BigHeat"))
sapply(list.files(pattern = ".R"), source)


#setwd(functioncode)
#sapply(list.files(), source)

setwd()
```

#Load data

The data is too large to load all at once and perform the filtering operation so the data will be loaded piecewise filtered then all the small pieces will be recombined as searching through the model becomes longer the further down the file it needs to go the file is broken into smaller chunks to keep loading times reasonable.


Load an example dataset to test the functions
```{r}
setwd(datafile)

ScanTest <- JustTheseRows("TrialMonitoringDataHH.csv", 1, 100000, Columns = 5, Header = T)
write_csv(ScanTest, "ScanTest.csv")

#Sample random Rows from the dataset
#SomeRows <- RandomRows("TrialMonitoringDataHH.csv", 10000,100,5)

```


Test the loading and saving of the data

ETL

Start Chunk Loading
Convert to time taking account of multiple timezones
Filter for the time period under investigation
Reshape into wide form

Loading - SAveMANYCSV/LOADmanyCSV
Conversion - COnvert to Time
Filter Filter Time
Reshape - use already existing functions.


Function1 <- 'ConvertToTime %>% FilterTime'

data <- LoadManyCSV("folder", Function1 )

```{r Load data from csv}

#These commands are used for chunked loading they do the following
#join on the timezone data
#Convert to time object with regards the timezone of the measurement
#Filter to only the hours required
Function4 <- substitute( 
  x %>% 
    left_join(., mutate(TimeZoneDF, 
                        `Location ID` = as.character(`Location ID`)), 
              by= "Location ID") %>% 
    ConvertToTime(., "Date and Time of capture") %>% #Convert the time column to time class
  FilterTime(.,"Date and Time of capture", Start = 16, End = 21)
  )



#Loads the timezone data into a data frame
TimeZoneDF <- read.csv("HalfHourlyDataSource.csv", check.names = FALSE) %>% mutate(TimeZoneCol = ifelse(`Data provider` == "Logica",
                                              "GMT",
                                              "Europe/London")) %>%
  select(-`Data provider`)


#SaveManyRDS("ScanTest.csv", "test2", 1e4)
TestOut <- LoadManyRDS("test2", Function4)

#MakeManyRDS("TrialMonitoringDataHH.csv", "test", 1e6)
TestOutWhole <- LoadManyRDS("test", Function4) 




#This needs to be simplified so that the joining of the timezone is not part of the function

test <- LoadManyRDS("test2") %>% 
    left_join(., mutate(TimeZoneDF, 
                        `Location ID` = as.character(`Location ID`)), 
              by= "Location ID") %>% 
  ConvertToTime(., "Date and Time of capture") %>% 
  FilterTime(.,"Date and Time of capture", Start = 16, End = 21)

```

#Create a test time series with missing data and use for Bigheat testing
```{r}

#Simple example
test <- data.frame(matrix(data = 1, ncol = 20, nrow=100)) # create a dataframe of all 1's
test[1:5, 1:5] <- NA #insert NA's at The top left corner
orderedheat(!is.na(test),order = "none", mergex = 1, mergey = 1) #visualise


#Another example timeseries.

#10 Id's run for 100 days from 01-01

set.seed(1999)
IDnums<- sample(1:20, replace = FALSE) #generate the ID's so that there is no structure within the blocks

#Each block starts and stops at different times. This will create NA's in the data set when it is put into long form

block1 <-data_frame(
          Dates = rep(seq(dmy("01-01-2016"),(dmy("01-01-2016") + days(100-1)), by = 1), times = 10),
          Value = rep(1:100, 10),
          ID = rep(IDnums[1:10], each = 100)
          )

block2 <-data_frame(
          Dates = rep(seq(dmy("01-03-2016"),(dmy("01-03-2016") + days(50-1)), by = 1), times = 5),
          Value = rep(1:50, 5),
          ID = rep(IDnums[11:15], each = 50)
          )


block3 <-data_frame(
          Dates = rep(seq(dmy("01-02-2016"),(dmy("01-02-2016") + days(100-1)), by = 1), times = 5),
          Value = rep(1:100, 5),
          ID = rep(IDnums[16:20], each = 100)
          )

#Combine the data frames together and then use dcast/spread to convert it to wide form
df <- bind_rows(block1,block2,block3) %>% 
  dcast(., ID~Dates, 
              value.var = "Value", 
              drop=TRUE)

#add in some additional NA values
df[1:nrow(df),c(2:5,73:77)] <- NA

#create a new data frame that shows 1 if valid data and 0 if NA
df2 <-!is.na(df[,-1])

#Visualise the data
orderedheat(df2,order = "none")

#Compression isn't necessary so change merge from default aggregating 5 cells to 1
orderedheat(df2,order = "none", mergey = 1, mergex = 1)

#See what a fully organised dataset would look like
orderedheat(df2,order = "both", mergey = 1, mergex = 1)

#In this case we want data in chronological order so we run again
orderedheat(df2,order = "row", mergey = 1, mergex = 1)

#If you want to have more than just the plot you can use the subfunction of big heat to return the order etc of the Rows and Columns
CellOrder <- createorder(df2)

#This allows us to see the groupings of the IDs, which can be useful when subsetting the data frame to find high quality zones.
df$ID[CellOrder$Roworder]
```



#After the data has been cleanded for the logica trilliant split, the data needs to be reshaped for a smartmeter per row and a time unit per column.

```{r isnaframe}
#data.table is used as it is better with larger table structures, this may stop being relevant with increased integration with dplyr
smartdata <- dcast(smartdata, Date.Time ~Location.ID, 
              value.var = "Parameter", 
              drop=FALSE)

setwd(SubDataSets)
saveRDS(smartdata,"smartdata.rds")
#smartdata <- readRDS("smartdata.rds")
min(smartdata$Date.Time)
max(smartdata$Date.Time)
max(smartdata$Date.Time)-min(smartdata$Date.Time)


#Create is na frame where 1 is valid and 0 is NA
isnaframe <- 1-is.na(smartdata[,-1])*1
saveRDS(isnaframe, "isnaframe.rds")
#isnaframe <- readRDS("isnaframe.rds")
rm(smartdata)

#How much data is missing as a percentage of total
1-sum(isnaframe)/(ncol(isnaframe)*nrow(isnaframe))

#create the row column ordering for isnaframe
#this takes much longer when it has to write to dropbox

ordering <- createorder(isnaframe, order="both", simMat= FALSE,xblocks=5, yblocks=5, par=TRUE)

saveRDS(ordering, "isnaordering.rds")
#ordering <- readRDS("isnaordering.rds")
```

Pre cleaning unordered
```{r precleaning plot}
test <- orderedheat(isnaframe, order = "none", merge = 5, simMat = FALSE,
                xblocks=10, yblocks=10, mid = 0.5, legend="Percent Valid")
test+     
    labs(x = "Date time",
         y = "Smartmeter ID") +ggtitle("Missing data pre-cleaning")

setwd(Figures)
ggsave("unorderedPrecleaningmissing.pdf")
rm(test)
#smartmeter % complete data

```


Pre cleaning ordered
```{r precleaning ordered}
test <- bigheat(isnaframe[ordering$Roworder,ordering$Colorder],
                merge = 5,mid=0.5, legend="Percent Valid")
test+     
    labs(x = "Date time",
         y = "Smartmeter ID") +ggtitle("Missing data pre-cleaning")
ggsave("Precleaningmissing.pdf")
rm(test)

```



highlighting smart meter groups in the correct time order. 
As there are two clear groups of smartmeters and a group of smart meters that have not delivered very good quality information, it is important to look at how the clusters behave in normal time
```{r extract smartmeters}

findbreak<- t(isnaframe[ordering$Roworder, ordering$Colorder[300:6000]]) %>% as.data.frame %>%
  mutate(rowsum = rowSums(.), 
         diff= lag(rowsum,1), 
         rowID= 1:nrow(.), 
         rM=(rowsum + lag(rowsum)+lead(rowsum))/3) %>% 
  select(rowsum, diff, rowID, rM)

ggplot(findbreak, aes(x = rowID, y = rowsum)) + geom_line() +
  ggtitle("Identifying break points in the smartmeter clusters") +
  labs(x="Cluster ordered MeterIDs", y = "Number of Valid data points")
ggsave("breakpoints.pdf")
#break point at groups at 1:2380 and 2381:4530 add list of smart meters in the appendix along with time periods
```


Lower Cluster shown in chronological time 
```{r plot valid meters}

#ensures aggregation happens correctly
lowerclustID <- 300:6000 #the smart meters to select

test <- bigheat(isnaframe[,ordering$Colorder[lowerclustID]],
                merge = 5,mid=0.5, legend="Percent Valid")
test+     
    labs(x = "Date time",
         y = "Smartmeter ID") +ggtitle("Missing data pre-cleaning")

ggsave("LowerPrecleaningmissing.pdf")

```


#Removing highly missing data

Now the data is broken into two clusters of smart meters the time componant can be filtered to leave  high quality data set.



Cleaning the cluster
```{r cleaningthedata}
setwd(SubDataSets)
#makes a matrix where 1 means there is data and 0 means NA
lowerclust <- isnaframe[, ordering$Colorder[lowerclustID]]%>% as.data.frame
saveRDS(lowerclust, "lowerclust.rds")
#lowerclust <- readRDS("lowerclust.rds")
lowertimepercs <- rowSums(lowerclust)/ncol(lowerclust)

setwd(Figures)


#create a data frame showing how many time periods have more than x% values
nonmissing <- data.frame(cutoff = seq(0.1,1,0.01), TimePeriods =NA, SmartMeters = NA)

nonmissing$TimePeriods <- sapply(nonmissing$cutoff ,function(n) {
  sum(lowertimepercs>n, na.rm = TRUE)
  })

ggplot(nonmissing, aes(x= cutoff, y= TimePeriods)) + geom_line() +ggtitle("Number of Time Periods that have at least \nthe percentage of valid data indicated by the cut off") +xlab("Cut Off") +ylab("Number of Valid Time Periods")
ggsave("NAtimeperiodslowerclust.pdf")


#Remove Time periods with less than 90% valid data
lowerclust <- lowerclust[lowertimepercs>0.9,]

lowermeterpercs <- colSums(lowerclust)/nrow(lowerclust)

nonmissing$SmartMeters <- sapply(nonmissing$cutoff ,function(n) {
  sum(lowermeterpercs>n, na.rm = TRUE)
  })

ggplot(nonmissing, aes(x= cutoff, y= SmartMeters)) + geom_line() +ggtitle("Number smart meters that have at least \nthe percentage of valid data indicated by the cut off") +xlab("Cut Off") +ylab("Number of Valid smart meters")
ggsave("NAsmartmeters.pdf")

#filter the meters
lowerclust <- lowerclust[,lowermeterpercs >0.99]
totalmeters <- sum(lowermeterpercs >0.99)

#How much data is missing as a percentage of total post cleaning
sum((lowerclust))/(ncol(lowerclust)*nrow(lowerclust))
rm(isnaframe)
rm(lowerclust)

setwd(SubDataSets)
smartdata <- readRDS("smartdata.rds")

#Check how many days are missing to have contiguous days from start to finish in the block

datevect <- as.Date(smartdata$Date.Time)[lowertimepercs>0.9] %>% unique
alldays <-seq(from=min(datevect) , to=max(datevect),by = "day") 
MissingDays <-alldays[!(alldays %in% unique(datevect))] 

#there are only three days missing for a full house make a vector to inlcude them as well
MissingDays <-as.Date(smartdata$Date.Time) %in% MissingDays

cleandata <-smartdata[,c(1,(1+ordering$Colorder[lowerclustID]))]
size <- ncol(cleandata)*nrow(cleandata)
cleandata <- cleandata[(lowertimepercs>0.9|MissingDays), c(TRUE,lowermeterpercs >0.99)]



ncol(cleandata)*nrow(cleandata)/size #amount of remaingin data

saveRDS(cleandata, "cleandata.rds")
#cleandata <-readRDS("cleandata.rds")
```

The result of cleaning both the cluster is that only minor smart meter removal specific removal needs to take place after the time periods have been cleaned up. This suggests that within the clusters data quality is strongly related related to time period and not to smart meter.

#Exploring the data

How many days are full days?
```{r}
fulldays <- cleandata %>% group_by(date(Date.Time)) %>% summarise(total = n()) %>%
  rename(Date.Time = `date(Date.Time)`)
table(fulldays$total)

```


how many days have date 1 day before and 7 days before?
```{r}
weekdiff <- fulldays$Date.Time -ddays(7)
sum(weekdiff %in% fulldays$Date.Time) #239 days

weekdiff <- fulldays$Date.Time -ddays(1)
sum(weekdiff %in% fulldays$Date.Time) #239 days

sum(is.na(cleandata))

sum(is.na(cleandata))/size

#The missingness of the days that have been included even though they don't make the cut.
day1<- cleandata %>% filter(as.Date(Date.Time)==ymd("2011-06-29")) %>%is.na %>% sum
1-day1/(12*5261)

day1<- cleandata %>% filter(as.Date(Date.Time)==ymd("2011-10-31")) %>%is.na %>% sum
1-day1/(12*5261)
day1<- cleandata %>% filter(as.Date(Date.Time)==ymd("2011-12-12")) %>%is.na %>% sum
1-day1/(12*5261)

```



Fill in missing values by day time average, then average by day using a three time period window
```{r fill in missing}
setwd(SubDataSets)
cleandata <- readRDS("cleandata.rds")
#add in missing row for day 177
missingrow <-matrix(NA, nrow=1,ncol=ncol(cleandata)) %>% as.data.frame %>%
  mutate_all(funs(as.numeric))
names(missingrow) <- names(cleandata)

missingrow <- missingrow  %>% mutate(Date.Time= as.POSIXct("2011-10-25 18:00:00", tz="Europe/London") + minutes(30))

cleandata <- cleandata %>% bind_rows(., missingrow) %>% 
  arrange(Date.Time)

#make a data frame of average day hour in values
dayhourmin <- paste(wday(cleandata$Date.Time),
                    hour(cleandata$Date.Time),
                    minute(cleandata$Date.Time),
                    sep=":")

meanvals <- cleandata[,-1] %>%
  mutate(time.day = dayhourmin) %>% group_by(time.day) %>%
  summarise_each(funs(mean(., na.rm=TRUE))) %>%ungroup

navect <- cleandata %>% is.na %>% which(., arr.ind=T)

NACols <- unique(navect[,2] )

for(i in 1:length(NACols)){
colID <-NACols[i]
rowIDs <- navect[navect[,2]==colID,1]

RowsFromMeanVals<- match(dayhourmin[rowIDs],meanvals$time.day)

cleandata[rowIDs,colID] <- meanvals[RowsFromMeanVals,colID] %>%unlist
if((i%%100)==0){print(i)}  
}

#check there are no Na values
cleandata %>% is.na %>% sum
saveRDS(cleandata, file="cleandatafilled.rds")
#cleandata <- readRDS("cleandatafilled.rds")


rm(list= c("NACols","dayhourmin", "i","RowsFromMeanVals", "navect", "meanvals", "colID", "rowIDs", "missingrow"))



```


```{r Internal Cor}
set.seed(1238)
NodeIDs <- sample(1:5260, 50)+1
IntCor <- cleandata[,c(1,NodeIDs)] %>%
  mutate(Time = paste(hour(cleandata$Date.Time),
                    minute(cleandata$Date.Time),
                    sep=":"),
         Date = as.Date(Date.Time)
)%>%  gather(key = NodeID, value = kwh, 
             -Date.Time, 
             -Time,
             -Date) %>% select(-Date.Time) %>%
  spread(key= Time, value=kwh)

IntCorList <- mclapply(unique(IntCor$NodeID), function(n){
  
  IntCor %>% filter(NodeID ==n) %>% select(-NodeID, -Date) %>%
    t %>%
    cor
  },
mc.cores=detectCores())

#visualise the corellation matrix of the first smart mater
IntCorVis <-orderedheat(IntCorList[[2]], order="both", simMat = TRUE, merge = 1, mid = 0)
IntCorVis

IntCorVis2 <-orderedheat(abs(IntCorList[[1]]), order="both", simMat = TRUE, merge = 1)
IntCorVis2

  MeanAbsCor <- sapply(1:50, function(n) mean(abs(IntCorList[[n]]))) %>% data.frame(value=., NodeID= NodeIDs)  
  
  ggplot(MeanAbsCor, aes(x= value)) + geom_density(fill="steelblue", alpha =0.7) +
    labs(title= "Mean absolute correlation for 50 nodes with themselves", 
         x= "Mean absolute Corellation" )
  setwd(file.path(Figures, "Results"))    
  ggsave("MeanAbsCorr.pdf")
  
  
  #Distribution of the data
  
cleandata %>% gather(. , key=SMartID, value=kWh, -Date.Time) %>%
  ggplot(., aes(x=kWh)) + geom_density(fill="steelblue", alpha =0.7) + ggtitle("Distribution of energy consumption")
ggsave("energydensity.pdf")
  
cleandata %>% gather(. , key=SMartID, value=kWh, -Date.Time) %>%
  ggplot(., aes(x=log10(kWh))) + geom_density(fill="steelblue", alpha =0.7) + ggtitle("Distribution of energy consumption")
  setwd(file.path(Figures, "Appendix")) 
ggsave("logenergydensity.pdf")


```

